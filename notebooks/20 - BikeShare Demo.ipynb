{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast use of a city bikeshare system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\n",
    "\n",
    "The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://kaggle2.blob.core.windows.net/competitions/kaggle/3948/media/bikes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **datetime** - hourly date + timestamp  \n",
    "* **season** -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "* **holiday** - whether the day is considered a holiday\n",
    "* **workingday** - whether the day is neither a weekend nor holiday\n",
    "* **weather** - \n",
    "    * 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n",
    "    * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n",
    "    * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered * **clouds**\n",
    "    * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "* **temp** - temperature in Celsius\n",
    "* **atemp** - \"feels like\" temperature in Celsius\n",
    "* **humidity** - relative humidity\n",
    "* **windspeed** - wind speed\n",
    "* **casual** - number of non-registered user rentals initiated\n",
    "* **registered** - number of registered user rentals initiated\n",
    "* **count** - number of total rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions are evaluated one the Root Mean Squared Logarithmic Error (RMSLE). The RMSLE is calculated as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "* $n$ is the number of hours in the test set\n",
    "* $p_i$ is your predicted count\n",
    "* $a_i$ is the actual count\n",
    "* $log(x)$ is the natural logarithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Kaggle Reference](https://www.kaggle.com/c/bike-sharing-demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Exploration**\n",
    "    1. Distributions (Univariate)\n",
    "    1. Correlations (Bivariate)\n",
    "    1. Plots (Multivariate)\n",
    "2. **Analysis**\n",
    "    1. Write the Scoring Method\n",
    "    1. Build a 'mean value' baseline model for reference\n",
    "    1. Set up crossvalidation pipeline\n",
    "    1. Build our first regression model\n",
    "    1. Feature Engineering\n",
    "    1. Tune parameters to improve the model\n",
    "3. **Submission**\n",
    "    1. Submit our predictions to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print pd.__name__, pd.__version__\n",
    "print np.__name__, np.__version__\n",
    "print mpl.__name__, mpl.__version__\n",
    "print sns.__name__, sns.__version__\n",
    "print sklearn.__name__, sklearn.__version__\n",
    "print sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def table(df,replace_match=\"\",replace_str=\"\"):\n",
    "    return IPython.display.display(HTML(df.to_html().replace('<table border=\"1\" class=\"dataframe\">','<table class=\"table table-striped table-hover\">').replace(replace_match,replace_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/bikeshare/'\n",
    "TRAIN_FILE = DATA_DIR + 'train.csv'\n",
    "TEST_FILE = DATA_DIR + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.datetime = pd.to_datetime(df.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table(df.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random sample of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "table(df.ix[sample(df.index,10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "with sns.plotting_context(\"poster\", font_scale=1, rc=c):\n",
    "    qqplot(df['windspeed'], line='45', fit=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b, g, r, p = sns.color_palette(\"muted\", 4)\n",
    "\n",
    "with sns.plotting_context(\"poster\", font_scale=1, rc=c):\n",
    "    g = sns.PairGrid(df, hue=\"workingday\")\n",
    "    g.map_diag(plt.hist)\n",
    "    g.map_offdiag(plt.scatter)\n",
    "    g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working Day vs. Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sns.plotting_context(\"poster\", font_scale=1, rc=c):\n",
    "    g = sns.FacetGrid(df, col=\"workingday\")\n",
    "    g.map(plt.hist, \"count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sns.plotting_context(\"poster\", font_scale=1, rc=c):\n",
    "    g = sns.FacetGrid(df, col=\"season\", size=4, aspect=.5)\n",
    "    g.map(sns.boxplot, \"atemp\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sns.plotting_context(\"poster\", font_scale=1, rc=c):\n",
    "    g = sns.FacetGrid(df, col=\"season\", hue=\"count\")\n",
    "    g.map(plt.scatter, \"temp\", \"atemp\", alpha=.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    # Loads the training data, but splits the y from the X\n",
    "    df = pd.read_csv(TRAIN_FILE)\n",
    "    return df.iloc[:, 0:9], df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# First, we should set up some sort of testing framework, so that we can benchmark our progress as we go\n",
    "# The evaluation metric is Root mean squared logarithmic error.\n",
    "def rmsele(actual, pred):\n",
    "    \"\"\"\n",
    "    Given a column of predictions and a column of actuals, calculate the RMSELE\n",
    "    \"\"\"\n",
    "    squared_errors = (np.log(pred + 1) - np.log(actual + 1)) ** 2\n",
    "    mean_squared = np.sum(squared_errors) / len(squared_errors)\n",
    "    return np.sqrt(mean_squared)\n",
    "\n",
    "# This helper function will make a callable that we can use in cross_val_score\n",
    "rmsele_scorer = make_scorer(rmsele, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "\n",
    "expected_value = df['count'].mean()\n",
    "yhat = np.array([expected_value] * len(df['count']))\n",
    "\n",
    "rmsele(df['count'].values,yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "\n",
    "# Lets just train a basic model so that we can test if our scoring and\n",
    "# cross validation framework works well. We'll use a Ridge regression,\n",
    "# which is a form of linear regression\n",
    "X, y = get_train_data()\n",
    "# Subset the X to just use temp, atemp, and workingday\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "ridge_estimator = Ridge(normalize=True)\n",
    "scores = cross_val_score(ridge_estimator, Xhat, y, scoring=rmsele_scorer, cv=5, verbose=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill in some of the parameters on cross_val_score\n",
    "def perform_cv(estimator, X, y):\n",
    "    return cross_val_score(estimator, X, y, scoring=rmsele_scorer, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Try a simple grid search with the estimator\n",
    "parameters = {'alpha': np.logspace(0, 2, 10)}\n",
    "grid = GridSearchCV(ridge_estimator, parameters, scoring=rmsele_scorer, cv=5)\n",
    "grid.fit(Xhat, y)\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And for grid_search\n",
    "def perform_grid_search(estimator, parameters, X, y):\n",
    "    grid_search = GridSearchCV(estimator, parameters, scoring=rmsele_scorer, cv=5)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Ridge to floor to Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Custom Ridge to floor predictions at 0\n",
    "class FlooredRidge(Ridge):\n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        pred = super(FlooredRidge, self).predict(X, *args, **kwargs)\n",
    "        pred[pred < 0] = 0\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xhat = X[['temp', 'atemp', 'humidity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "normalize.fit(Xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print normalize.std_\n",
    "print normalize.mean_\n",
    "(Xhat - Xhat.mean()) / Xhat.std()\n",
    "(Xhat - normalize.mean_) / normalize.std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Now lets move on to the actual transformation of the inputs\n",
    "# First, not every estimator we'll use will have the \"normalize\" keyword\n",
    "# So let's break it out into a transformer, so that we have better control over it\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "Xhat = normalize.fit_transform(Xhat)\n",
    "scores = perform_cv(ridge_estimator, Xhat, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# Now we have the beginnings of a multi-step pipeline\n",
    "# Scikit lets you wrap each of these steps into a Pipeline object, so you just have to run fit / predict once\n",
    "# instead of manually feeding the data from one transformer to the next\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "pipeline = Pipeline([('normalize', normalize), ('ridge', ridge_estimator)])\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "scores = perform_cv(pipeline, Xhat, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Additionally, you can perform grid search over all of the steps of the pipeline\n",
    "# So you don't have to tune each step manually\n",
    "# The pipeline exposes the underlying steps' parameters like so:\n",
    "# ridge__alpha, and normalize__with_mean\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "parameters = {'ridge__alpha': np.logspace(0, 3, 10)}\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "pipeline = Pipeline([('normalize', normalize), ('ridge', ridge_estimator)])\n",
    "grid = GridSearchCV(pipeline, parameters, scoring=rmsele_scorer, cv=5)\n",
    "grid.fit(Xhat, y)\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelBinarizer\n",
    "# Lets move on to including more features in our model\n",
    "# We probably want to use a factor like Season in our model, but it's\n",
    "# a categorical feature, and we'll need to convert it to a series of booleans\n",
    "one_hot = OneHotEncoder()\n",
    "season = one_hot.fit_transform(X['season'].reshape(X.shape[0], 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_hot.fit_transform(X['season'].reshape(X.shape[0], 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We then have to join this with the other variables\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "pipeline = Pipeline([('normalize', normalize), ('ridge', ridge_estimator)])\n",
    "Xhat = np.hstack([X[['temp', 'atemp', 'humidity']], season])\n",
    "scores = perform_cv(pipeline, Xhat, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Actually there's a faster way of doing this with the argument 'categorical_features'\n",
    "class ToArray(BaseEstimator, TransformerMixin):\n",
    "    # We need this because OneHotEncoder returns a sparse array, and normalize requires a non-sparse array\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.toarray()\n",
    "        \n",
    "Xhat = X[['season', 'weather', 'temp', 'atemp', 'humidity']]\n",
    "# I think it needs to be 5 here, because it assumes that '0' is a possible value for an int datatype\n",
    "# Should probably specify the data types in read_csv\n",
    "one_hot = OneHotEncoder(n_values=[5, 5], categorical_features=[0, 1])\n",
    "desparse = ToArray()\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = FlooredRidge()\n",
    "pipeline = Pipeline([('onehot', one_hot), ('desparse', desparse), ('normalize', normalize), ('ridge', ridge_estimator)])\n",
    "scores = perform_cv(pipeline, Xhat, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OK, so now we've got a pipeline that does one-hot encoding of two categorical variables\n",
    "# and then normalizes the variables\n",
    "# But actually we're not supposed to normalize the the dummy variables.\n",
    "# So we need some way of only normalizing non-dummy variables\n",
    "\n",
    "# Oops, actually the CV splitting converts the Pandas DF to an array, so we can't rely\n",
    "# on the normalize having the proper column names\n",
    "class SelectiveNormalize(StandardScaler):\n",
    "    def __init__(self, cols, copy=True, with_mean=True, with_std=True):\n",
    "        self.cols = cols\n",
    "        super(SelectiveNormalize, self).__init__(copy, with_mean, with_std)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        subset = X[:, self.cols]\n",
    "        return super(SelectiveNormalize, self).fit(subset, y)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        subset = X[:, self.cols]\n",
    "        normalized = super(SelectiveNormalize, self).transform(subset)\n",
    "        others = [col for col in range(X.shape[1]) if col not in self.cols]\n",
    "        res = np.hstack([normalized, X[:, others]])\n",
    "        return res\n",
    "\n",
    "Xhat = X[['season', 'weather', 'temp', 'atemp', 'humidity']]\n",
    "one_hot = OneHotEncoder(n_values=[5, 5], categorical_features=[3, 4])\n",
    "normalize = SelectiveNormalize([2, 3, 4])\n",
    "desparse = ToArray()\n",
    "ridge_estimator = FlooredRidge()\n",
    "pipeline = Pipeline([('normalize', normalize), ('onehot', one_hot), ('desparse', desparse), ('ridge', ridge_estimator)])\n",
    "scores = perform_cv(pipeline, Xhat, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Lets try tackling the date column now.  The time of day is probably really important\n",
    "# So we need some way of extracting the hour\n",
    "# We'll use a FeatureUnion to do this, to demonstrate the functionality\n",
    "def get_train_data():\n",
    "    # Loads the training data, but splits the y from the X\n",
    "    df = pd.read_csv(TRAIN_FILE, parse_dates=['datetime'])\n",
    "    return df.iloc[:, 0:9], df.iloc[:,-1]\n",
    "\n",
    "\n",
    "class SelectColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Passes on a subset of columns from an input ndarray\n",
    "    \"\"\"\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.cols]\n",
    "    \n",
    "\n",
    "class ExtractHour(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extracts hour from a datetime series\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        res = np.zeros(X.shape)\n",
    "        for xx in xrange(X.shape[0]):\n",
    "            res[xx] = X[xx, 0].hour\n",
    "        return res.reshape(res.shape[0], 1)\n",
    "    \n",
    "\n",
    "class CastType(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cast_to):\n",
    "        self.cast_to = cast_to\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.astype(self.cast_to)\n",
    "\n",
    "X, y = get_train_data()\n",
    "# Reminder of the columns:\n",
    "# ['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed']\n",
    "select_date = SelectColumns([0])\n",
    "select_others = SelectColumns(range(1, 9))\n",
    "cast_float = CastType(np.float64)\n",
    "one_hot = OneHotEncoder(n_values=[5, 5], categorical_features=[0, 3])\n",
    "get_hour = ExtractHour()\n",
    "normalize = SelectiveNormalize(range(2, 8))\n",
    "desparse = ToArray()\n",
    "ridge_estimator = RandomForestRegressor(n_estimators=200)\n",
    "\n",
    "hour_feature = Pipeline([('select_date', select_date), ('get_hour', get_hour)])\n",
    "other_features = Pipeline([('select_others', select_others), ('cast_float', cast_float), ('onehot', one_hot), ('desparse', desparse)])\n",
    "join_features = FeatureUnion([('hour', hour_feature), ('others', other_features)])\n",
    "predict = Pipeline([('featurize', join_features), ('estimator', ridge_estimator)])\n",
    "scores = perform_cv(predict, X, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submisison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "\n",
    "code_show=true;\n",
    "\n",
    "function code_toggle() {\n",
    "    if (code_show){ \n",
    "        $('div.input').hide();\n",
    "        $('.output_scroll').removeClass('output_scroll');\n",
    "        $('.prompt').hide();\n",
    "    } else {\n",
    "        $('div.input').show();\n",
    "        $('.output_scroll').removeClass('output_scroll');\n",
    "        $('.prompt').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "}\n",
    "</script>\n",
    " \n",
    "<a class='btn btn-warning btn-lg' style=\"margin:0 auto; display:block; max-width:320px\" href=\"javascript:code_toggle()\">TOGGLE CODE</a>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML('''<link href='http://fonts.googleapis.com/css?family=Roboto|Open+Sans' rel='stylesheet' type='text/css'>\n",
    "<style>\n",
    "body #notebook {\n",
    "    font-family : 'Open Sans','Source Sans Pro','Proxima Nova', sans-serif;\n",
    "    font-size : 1.3em;\n",
    "    line-height : 1.5em;\n",
    "}\n",
    "\n",
    "h1,h2,h3,h4,h5 {\n",
    "    font-family : 'Roboto','Source Sans Pro','Proxima Nova', sans-serif;\n",
    "}\n",
    "\n",
    "\n",
    "#notebook .panel-body {\n",
    "  font-size: 1.1em;\n",
    "  line-height: 1.6em;\n",
    "}\n",
    "\n",
    "#notebook .table,\n",
    "#notebook .table th,\n",
    "#notebook .table td,\n",
    "#notebook .table tr {\n",
    "    text-align : center;\n",
    "    border: 0;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<script>\n",
    "$(function(){\n",
    "    code_toggle()\n",
    "})\n",
    "</script>\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "TEST_FILE = 'data/test.csv'\n",
    "TRAIN_FILE = 'data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distibutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "pd.options.display.mpl_style = 'default'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.hist(figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.boxplot(column='atemp', by='season');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.boxplot(figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['windspeed'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "qqplot(df['windspeed'], line='45', fit=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "scatter_matrix(df, alpha=0.2, figsize=(12, 12), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    # Loads the training data, but splits the y from the X\n",
    "    df = pd.read_csv(TRAIN_FILE)\n",
    "    return df.iloc[:, 0:9], df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# First, we should set up some sort of testing framework, so that we can benchmark our progress as we go\n",
    "# The evaluation metric is Root mean squared logarithmic error.\n",
    "def rmsele(actual, pred):\n",
    "    \"\"\"\n",
    "    Given a column of predictions and a column of actuals, calculate the RMSELE\n",
    "    \"\"\"\n",
    "    squared_errors = (np.log(pred + 1) - np.log(actual + 1)) ** 2\n",
    "    mean_squared = np.sum(squared_errors) / len(squared_errors)\n",
    "    return np.sqrt(mean_squared)\n",
    "\n",
    "# This helper function will make a callable that we can use in cross_val_score\n",
    "rmsele_scorer = make_scorer(rmsele, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "\n",
    "# Lets just train a basic model so that we can test if our scoring and\n",
    "# cross validation framework works well. We'll use a Ridge regression,\n",
    "# which is a form of linear regression\n",
    "X, y = get_train_data()\n",
    "# Subset the X to just use temp, atemp, and workingday\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "ridge_estimator = Ridge(normalize=True)\n",
    "scores = cross_val_score(ridge_estimator, Xhat, y, scoring=rmsele_scorer, cv=5, verbose=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill in some of the parameters on cross_val_score\n",
    "def perform_cv(estimator, X, y):\n",
    "    return cross_val_score(estimator, X, y, scoring=rmsele_scorer, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Try a simple grid search with the estimator\n",
    "parameters = {'alpha': np.logspace(0, 2, 10)}\n",
    "grid = GridSearchCV(ridge_estimator, parameters, scoring=rmsele_scorer, cv=5)\n",
    "grid.fit(Xhat, y)\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And for grid_search\n",
    "def perform_grid_search(estimator, parameters, X, y):\n",
    "    grid_search = GridSearchCV(estimator, parameters, scoring=rmsele_scorer, cv=5)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Ridge to floor to Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Custom Ridge to floor predictions at 0\n",
    "class FlooredRidge(Ridge):\n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        pred = super(FlooredRidge, self).predict(X, *args, **kwargs)\n",
    "        pred[pred < 0] = 0\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lets move on to the actual transformation of the inputs\n",
    "# First, not every estimator we'll use will have the \"normalize\" keyword\n",
    "# So let's break it out into a transformer, so that we have better control over it\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "Xhat = normalize.fit_transform(Xhat)\n",
    "scores = perform_cv(ridge_estimator, Xhat, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# Now we have the beginnings of a multi-step pipeline\n",
    "# Scikit lets you wrap each of these steps into a Pipeline object, so you just have to run fit / predict once\n",
    "# instead of manually feeding the data from one transformer to the next\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "pipeline = Pipeline([('normalize', normalize), ('ridge', ridge_estimator)])\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "scores = perform_cv(pipeline, Xhat, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Additionally, you can perform grid search over all of the steps of the pipeline\n",
    "# So you don't have to tune each step manually\n",
    "# The pipeline exposes the underlying steps' parameters like so:\n",
    "# ridge__alpha, and normalize__with_mean\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "parameters = {'ridge__alpha': np.logspace(0, 3, 10)}\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "pipeline = Pipeline([('normalize', normalize), ('ridge', ridge_estimator)])\n",
    "grid = GridSearchCV(pipeline, parameters, scoring=rmsele_scorer, cv=5)\n",
    "grid.fit(Xhat, y)\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelBinarizer\n",
    "# Lets move on to including more features in our model\n",
    "# We probably want to use a factor like Season in our model, but it's\n",
    "# a categorical feature, and we'll need to convert it to a series of booleans\n",
    "one_hot = OneHotEncoder()\n",
    "season = one_hot.fit_transform(X['season'].reshape(X.shape[0], 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We then have to join this with the other variables\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "pipeline = Pipeline([('normalize', normalize), ('ridge', ridge_estimator)])\n",
    "Xhat = np.hstack([X[['temp', 'atemp', 'humidity']], season])\n",
    "scores = perform_cv(pipeline, Xhat, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Actually there's a faster way of doing this with the argument 'categorical_features'\n",
    "class ToArray(BaseEstimator, TransformerMixin):\n",
    "    # We need this because OneHotEncoder returns a sparse array, and normalize requires a non-sparse array\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.toarray()\n",
    "        \n",
    "Xhat = X[['season', 'weather', 'temp', 'atemp', 'humidity']]\n",
    "# I think it needs to be 5 here, because it assumes that '0' is a possible value for an int datatype\n",
    "# Should probably specify the data types in read_csv\n",
    "one_hot = OneHotEncoder(n_values=[5, 5], categorical_features=[0, 1])\n",
    "desparse = ToArray()\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = FlooredRidge()\n",
    "pipeline = Pipeline([('onehot', one_hot), ('desparse', desparse), ('normalize', normalize), ('ridge', ridge_estimator)])\n",
    "scores = perform_cv(pipeline, Xhat, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OK, so now we've got a pipeline that does one-hot encoding of two categorical variables\n",
    "# and then normalizes the variables\n",
    "# But actually we're not supposed to normalize the the dummy variables.\n",
    "# So we need some way of only normalizing non-dummy variables\n",
    "\n",
    "# Oops, actually the CV splitting converts the Pandas DF to an array, so we can't rely\n",
    "# on the normalize having the proper column names\n",
    "class SelectiveNormalize(StandardScaler):\n",
    "    def __init__(self, cols, copy=True, with_mean=True, with_std=True):\n",
    "        self.cols = cols\n",
    "        super(SelectiveNormalize, self).__init__(copy, with_mean, with_std)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        subset = X[:, self.cols]\n",
    "        return super(SelectiveNormalize, self).fit(subset, y)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        subset = X[:, self.cols]\n",
    "        normalized = super(SelectiveNormalize, self).transform(subset)\n",
    "        others = [col for col in range(X.shape[1]) if col not in self.cols]\n",
    "        res = np.hstack([normalized, X[:, others]])\n",
    "        return res\n",
    "\n",
    "Xhat = X[['season', 'weather', 'temp', 'atemp', 'humidity']]\n",
    "one_hot = OneHotEncoder(n_values=[5, 5], categorical_features=[3, 4])\n",
    "normalize = SelectiveNormalize([2, 3, 4])\n",
    "desparse = ToArray()\n",
    "ridge_estimator = FlooredRidge()\n",
    "pipeline = Pipeline([('normalize', normalize), ('onehot', one_hot), ('desparse', desparse), ('ridge', ridge_estimator)])\n",
    "scores = perform_cv(pipeline, Xhat, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Lets try tackling the date column now.  The time of day is probably really important\n",
    "# So we need some way of extracting the hour\n",
    "# We'll use a FeatureUnion to do this, to demonstrate the functionality\n",
    "def get_train_data():\n",
    "    # Loads the training data, but splits the y from the X\n",
    "    df = pd.read_csv(TRAIN_FILE, parse_dates=['datetime'])\n",
    "    return df.iloc[:, 0:9], df.iloc[:,-1]\n",
    "\n",
    "\n",
    "class SelectColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Passes on a subset of columns from an input ndarray\n",
    "    \"\"\"\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.cols]\n",
    "    \n",
    "\n",
    "class ExtractHour(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extracts hour from a datetime series\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        res = np.zeros(X.shape)\n",
    "        for xx in xrange(X.shape[0]):\n",
    "            res[xx] = X[xx, 0].hour\n",
    "        return res.reshape(res.shape[0], 1)\n",
    "    \n",
    "\n",
    "class CastType(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cast_to):\n",
    "        self.cast_to = cast_to\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.astype(self.cast_to)\n",
    "\n",
    "X, y = get_train_data()\n",
    "# Reminder of the columns:\n",
    "# ['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed']\n",
    "select_date = SelectColumns([0])\n",
    "select_others = SelectColumns(range(1, 9))\n",
    "cast_float = CastType(np.float64)\n",
    "one_hot = OneHotEncoder(n_values=[5, 5], categorical_features=[0, 3])\n",
    "get_hour = ExtractHour()\n",
    "normalize = SelectiveNormalize(range(2, 8))\n",
    "desparse = ToArray()\n",
    "ridge_estimator = RandomForestRegressor(n_estimators=200)\n",
    "\n",
    "hour_feature = Pipeline([('select_date', select_date), ('get_hour', get_hour)])\n",
    "other_features = Pipeline([('select_others', select_others), ('cast_float', cast_float), ('onehot', one_hot), ('desparse', desparse)])\n",
    "join_features = FeatureUnion([('hour', hour_feature), ('others', other_features)])\n",
    "predict = Pipeline([('featurize', join_features), ('estimator', ridge_estimator)])\n",
    "scores = perform_cv(predict, X, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = get_train_data()\n",
    "X['datetime'].apply(lambda xx: xx.hour)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

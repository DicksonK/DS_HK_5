{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = false\n",
       "var theme_url = \"https://drostehk.github.io/ipynb-theme/\";\n",
       "var asset_url = 'https://raw.githubusercontent.com/tijptjik/DS_assets/master/';\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_url + 'custom.js'\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = false\n",
    "var theme_url = \"https://drostehk.github.io/ipynb-theme/\";\n",
    "var asset_url = 'https://raw.githubusercontent.com/tijptjik/DS_assets/master/';\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_url + 'custom.js'\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If there are two courses of action, you should take the third.\n",
    "\n",
    "<footer>Jewish Proverb</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/agenda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Decision Trees](#Decision-Trees)\n",
    "1. [Building Decision Trees](#Building-Decision-Trees)\n",
    "1. [Preventing Overfitting](#Preventing-Overfitting)\n",
    "\n",
    "**Labs**\n",
    "1. [Implementing Decision Trees With Scikit-learn](#Implementing-Decision-Trees-With-Scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/mountain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Imbalance is the problem that the total number of a class of data (positive) is far less than the total number of another class of data (negative). This can be a problem because when you train a model, it may become really good at finding the majority class, but far weaker at capturing the minority class - while the minority case is often what's most interesting.\n",
    "\n",
    "![](assets/dataset-partition.png)\n",
    "\n",
    "The approached for addressing this are:\n",
    "* **Sampling based** : By **oversampling** the minority class, it has more effect on the machine learning algorithm, or by **undersampling** majority class so it has less effect on the machine learning algorithm.\n",
    "* **Cost function based approaches** : Make it more 'costly' for the learning algorithm to make mistakes in the minority class. This is done by tweaking the **cost function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/theory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree](assets/tree.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a flowchart-like structure in which each internal node represents a test of an attribute, each branch represents an outcome of that test and each leaf node represents class label (a decision taken after testing all attributes in the path from the root to the leaf). Each path from the root to a leaf can also be represented as a classification rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Non-parametric**: no parameters, no distribution assumptions\n",
    "* **Hierarchical**: consists of a sequence of questions which yield a class label when applied to any record\n",
    "* **Variable Size**: Any boolean functions can be represented\n",
    "* **Deterministic**: For the same set of features the tree will assign the same label\n",
    "* Support for **Discrete** and **Continuous** Parameters:\n",
    "  * Binning and Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a decision tree, the nodes represent questions (test conditions) and the edges are the answers to these questions.\n",
    "\n",
    "* *Edge*, lead from one a _parent_ to _child_ nodes\n",
    "* *Root*, node has 0 incoming edges, and 2+ outgoing edges.\n",
    "* *Leaf*, has 1 incoming edge and, 0 outgoing edges. Leaf nodes\n",
    "correspond to class labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hypertriangles](assets/hypertriangles.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/vertedrate_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/decision_tree_mammal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal nodes represent test conditions which partition the records at that node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/decision_boundaries.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a decision tree, one possibility would be to evaluate all possible decision trees (eg, all permutations of test conditions) for a given dataset. But this is generally too complex to be practical à $ \\rightarrow O(2^n)$.\n",
    "\n",
    "We can find a practical solution that works, by using a **heuristic** algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic method used to build (or “grow”) a decision tree is **Hunt’s\n",
    "algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hunt’s is a greedy recursive algorithm that leads to a local optimum. It builds a decision tree by recursively partitioning records into smaller & smaller subsets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* greedy – algorithm makes locally optimal decision at each step\n",
    "* recursive – splits task into subtasks, solves each the same way\n",
    "* local optimum – solution for a given neighborhood of points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partitioning decision is made at each node according to a metric called purity. A partition is 100% pure when all of its records belong to a single class. Let $D_t$ be the set of training records that reach a node $t$. The general recursive procedure is defined as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If $D_t$ only contains records that belong the same class $y_t$, then $t$ is a leaf node labeled as $y_t$\n",
    "* If $D_t$ is an empty set, then $t$ is a leaf node labeled by the default class, $y_d$\n",
    "* If $D_t$ contains records that belong to more than one class, use an attribute test to split the data into smaller subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It recursively applies the procedure to each subset until all the records in the subset belong to the same class. The Hunt's algirithm assumes that each combination of attribute sets has a unique class label during the procedure. If all the records associated with $D_t$ have identical attribute values except for the class label, then it is not possible to split these records any further. In this case, the node is declared a leaf node with the same class label as the majority class of training records associated with this node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Binary attributes* : leads to two-way split test condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dt_binary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Nominal attributes* : the test condition can be expressed into multiway split on each distinct values, or two-way split by grouping the attribute values into two subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dt_nominal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Ordinal attributes* : can also produce binary or multiway splits as long as the grouping does not violate the order property of the attribute values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dt_ordinal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Continuous attributes* : The test condition can be expressed as a comparsion test with two outcomes, or a range query. Or we can discretize the continous value into nominal attribute and then perform two-way or multi-way split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dt_continuous.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardest part about building the tree is selecting the best attribute test condition, in other words, the best split. There are three common impurity measures used to measure the best split. Since the goal of a decision tree is to have nodes consisting entirely of members of a single class, the impurity of a node is the extent to which that is not the case. For example, a node with 2 members of one class, and 0 members of another class has zero impurity. A node with 1 member of one class, and one of another, however, has the highest impurity. The three most common measures of impurity are entropy, gini impurity, and classification error. They are defined using the following equations, where $p(i|t)$ denotes the fraction of records that belong to class $i$ at a given node $t$, and $c$ is the number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "node_a = (12,8)\n",
    "node_b = (2,18)\n",
    "purity = lambda x : np.max(x) / np.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Entropy(t)=-\\sum\\limits_{i=0}^{c}p(i|t)\\log_{2} p(i|t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purity @ node_a : 0.6\n",
      "purity @ node_b : 0.9\n",
      "entroy of split : 0.884358712999\n",
      "entroy of split : 0.273605568201\n"
     ]
    }
   ],
   "source": [
    "print \"purity @ node_a :\", purity(node_a)\n",
    "print \"purity @ node_b :\", purity(node_b)\n",
    "entropy_a = -1 * (purity(node_a) * np.log2(purity(node_a)))\n",
    "entropy_b = -1 * (purity(node_b) * np.log2(purity(node_b)))\n",
    "\n",
    "print \"entroy of split :\", entropy_a * 2\n",
    "print \"entroy of split :\", entropy_b * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Gini(t)=1-\\sum\\limits_{i=0}^{c} [p(i|t)]^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purity @ node_a : 0.6\n",
      "purity @ node_b : 0.9\n",
      "gini of node_a : 0.48\n",
      "gini of node_b : 0.18\n"
     ]
    }
   ],
   "source": [
    "print \"purity @ node_a :\", purity(node_a)\n",
    "print \"purity @ node_b :\", purity(node_b)\n",
    "gini_a = 1 - np.sum([np.square(node) for node in [purity(node_a), 1 - purity(node_a)]])\n",
    "gini_b = 1 - np.sum([np.square(node) for node in [purity(node_b), 1 - purity(node_b)]])\n",
    "\n",
    "print \"gini of node_a :\", gini_a\n",
    "print \"gini of node_b :\", gini_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Classification\\ error(t)=1-\\max_{i}[p(i|t)] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purity @ node_a : 0.6\n",
      "purity @ node_b : 0.9\n",
      "classification error of node_a : 0.4\n",
      "classification error of node_b : 0.1\n"
     ]
    }
   ],
   "source": [
    "print \"purity @ node_a :\", purity(node_a)\n",
    "print \"purity @ node_b :\", purity(node_b)\n",
    "class_error_a = 1 - purity(node_a)\n",
    "class_error_b = 1 - purity(node_b)\n",
    "\n",
    "print \"classification error of node_a :\", class_error_a\n",
    "print \"classification error of node_b :\", class_error_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/impurity_measures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for information gain is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Delta = I(parent) - \\sum\\limits_{j=1}^k \\frac{N(v_j)}{N}I(v_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $I(⋅)$ is the impurity measure of a given node, $N$ is the total number or records at the given node's parent, $k$ is the number of attribute values, and $N(v_j)$ is the number of records associated with the child node, $v_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def purity(split):\n",
    "    return np.max(split)/np.sum(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gini(split):\n",
    "    a = purity(split)\n",
    "    b = 1 - a\n",
    "    gini = 1 - np.sum(np.square(a) + np.square(b))\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parent_node(nodes):\n",
    "    return [np.sum(x) for x in zip(nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def info_gain(nodes):\n",
    "    return gini(parent_node(nodes)) - sum([(np.sum(node)/np.sum(parent_node(nodes))) * gini(node) for node in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes = [node_a,node_b]\n",
    "\n",
    "def info_gain(nodes):\n",
    "    a = nodes[0]\n",
    "    b = nodes[1]\n",
    "    gini_parent = gini(parent_node(nodes))\n",
    "    x = (np.sum(a) / np.sum(parent_node(nodes))) * gini(a)\n",
    "    y = (np.sum(b) / np.sum(parent_node(nodes))) * gini(b)\n",
    "    return gini_parent - np.sum([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain: 0.170\n"
     ]
    }
   ],
   "source": [
    "ig = info_gain([node_a,node_b])\n",
    "print \"Information Gain: %.03f\" % ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Binary Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dt_splitting_binary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.489795918367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37142857142857139"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print gini((4,3))\n",
    "(gini((1,4)) * 5 + gini((5,2)) * 7) / 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Nominal Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dt_splitting_nominal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Continous Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dt_splitting_continuous.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, a test condition with a high number of outcomes\n",
    "can lead to overfitting (ex: a split with one outcome per record).\n",
    "One way of dealing with this is to restrict the algorithm to binary\n",
    "splits only (CART). Another way is to use a splitting criterion which explicitly penalizes the number of outcomes (C4.5). We can use a function of the information gain called the gain ratio to explicitly penalize high numbers of outcomes.\n",
    "\n",
    "Gain ratio is a modification of the information gain that reduces its bias on high-branch attributes. It will be\n",
    "\n",
    "* Large when data is evenly spread\n",
    "* Small when all data belong to one branch\n",
    "\n",
    "But the Gain Ratio also takes the number and size of branches into account when choosing an attribute. It corrects the information gain by taking the _intrinsic information_ of a split into account. That is, how much info do we need to tell which branch an instance belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Gain ratio = \\frac{\\Delta info}{Split Info} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Split Info = -\\sum\\limits_{i=0}^{k}P(v_i)log_2 P(v_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $p(v_i)$ refers to the probability of label $i$ at node $v$ and $k$ is the total number of splits. For example, if each attribute value has the same number of records, then $∀_i : P(v_i) = 1/k $ and the split information would be equal to $log_{2}k$. This example suggests that if an attribute produces a large number of splits, its split information will also be large , which in turn reduces its gain ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preventing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to determining splits, we also need a stopping criterion to\n",
    "tell us when we’re done. For example, we can stop when all records belong to the same class,\n",
    "or when all records have the same attributes. This is correct in principle, but would likely lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possibility is pre-pruning, which involves setting a minimum\n",
    "threshold on the gain, and stopping when no split achieves a gain\n",
    "above this threshold.\n",
    "\n",
    "This prevents overfitting, but is difficult to calibrate in practice (may\n",
    "preserve bias!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post-pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we could build the full tree, and then perform pruning\n",
    "as a post-processing step.\n",
    "\n",
    "To prune a tree, we examine the nodes from the bottom-up and\n",
    "simplify pieces of the tree (according to some criteria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complicated subtrees can be replaced either with a single node, or\n",
    "with a simpler (child) subtree.\n",
    "\n",
    "The first approach is called **subtree replacement**, and the second is\n",
    "**subtree raising**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dt_post_pruning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, (or at least depending on your data), it can be very easy to overfit a model with decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dt_overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values\n",
    "\n",
    "* Imputing during training\n",
    "  * Most frequent one in the dataset\n",
    "  * Most frequent in its class\n",
    "  * Fractional Examples, proportional to the real distribution\n",
    "\n",
    "\n",
    "* Imputing during testing\n",
    "  * Voting by fractional leafs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree algorithms: ID3, C4.5 and CART\n",
    "\n",
    "What are all the various decision tree algorithms and how do they differ from each other? Scikit-learn uses an optimised version of the CART algorithm.\n",
    "\n",
    "* ID3 (Iterative Dichotomiser 3) algorithm builds tree based on the information (information gain) obtained from the training instances and then uses the same to classify the test data. ID3 algorithm generally uses nominal attributes for classification with no missing values. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
    "\n",
    "* C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.\n",
    "\n",
    "* CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Decision Trees With Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement the decision tree classification to the test iris set\n",
    "* Review the implementation and output of a confusion matrix\n",
    "* Error terms: Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris implementation (as per usual):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points : 0\n",
      "Absolutely ridiculously overfit score: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, metrics, tree, cross_validation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "y_pred = clf.fit(iris.data, iris.target).predict(iris.data)\n",
    "\n",
    "print \"Number of mislabeled points : %d\" % (iris.target != y_pred).sum()\n",
    "print \"Absolutely ridiculously overfit score: %d\" % (tree.DecisionTreeClassifier().fit(iris.data,\n",
    "    iris.target).score(iris.data, iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEFORE we discuss the over-the-top overfitting with this model, get acquainted\n",
    "with two more important views of error to understand the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices allow you to view actual vs. predicted for all class labels.\n",
    "Since this model seems to be \"perfect,\" we can use this an an example of how to\n",
    "read a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  0,  0],\n",
       "       [ 0, 50,  0],\n",
       "       [ 0,  0, 50]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(iris.target, clf.predict(iris.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the identity matrix from earlier in class, a \"perfect\" prediction\n",
    "will result in complete matches at the diagonal and zeroes in all other places.\n",
    "However, this is not typical, so expect to see mismatches by reading it **predicted** (vertical) vs **actual** (horizontal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall are best explained as follows:\n",
    "\n",
    "* **Precision**: Of predicted value X, how many were actually X?\n",
    "(translation: of all predicted cats in a photo of cats and dogs, how many were actually cats?)\n",
    "* **Recall**: Of all the possible Xs, how many did you find?\n",
    "(translation, of all the possible cats in the photo, how many did you find?)\n",
    "\n",
    "Conveniently, we can use precision and recall with sklearn. (In a binary example,\n",
    "we can even create a different AUC using [precision and recall](http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under Curve: 0.82\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmP8P6HVVe36UDHcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd839e50f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn import metrics\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X, y = X[y != 2], y[y != 2]  # Keep also 2 classes (0 and 1)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "p = range(n_samples)  # Shuffle samples\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(p)\n",
    "\n",
    "X, y = X[p], y[p]\n",
    "half = int(n_samples / 2)\n",
    "\n",
    "# Add noisy features\n",
    "np.random.seed(0)\n",
    "X = np.c_[X, np.random.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# Run classifier\n",
    "classifier = svm.SVC(kernel='linear', probability=True, random_state=0)\n",
    "probas_ = classifier.fit(X[:half], y[:half]).predict_proba(X[half:])\n",
    "\n",
    "# Compute Precision-Recall and plot curve\n",
    "precision, recall, thresholds = precision_recall_curve(y[half:], probas_[:, 1])\n",
    "area = auc(recall, precision)\n",
    "print(\"Area Under Curve: %0.2f\" % area)\n",
    "\n",
    "pl.clf()\n",
    "pl.plot(recall, precision, label='Precision-Recall curve')\n",
    "pl.xlabel('Recall')\n",
    "pl.ylabel('Precision')\n",
    "pl.ylim([0.0, 1.05])\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.title('Precision-Recall example: AUC=%0.2f' % area)\n",
    "pl.legend(loc=\"lower left\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.76      0.75        25\n",
      "          1       0.75      0.72      0.73        25\n",
      "\n",
      "avg / total       0.74      0.74      0.74        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print metrics.classification_report(y[half:], classifier.predict(X[half:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalizing our tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, having a perfect model is more often _scary_. We know this\n",
    "is overfitting by using a test/train set from iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36  0  0]\n",
      " [ 0 34  0]\n",
      " [ 0  0 35]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        36\n",
      "          1       1.00      1.00      1.00        34\n",
      "          2       1.00      1.00      1.00        35\n",
      "\n",
      "avg / total       1.00      1.00      1.00       105\n",
      "\n",
      "[[14  0  0]\n",
      " [ 0 14  2]\n",
      " [ 0  2 13]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       0.88      0.88      0.88        16\n",
      "          2       0.87      0.87      0.87        15\n",
      "\n",
      "avg / total       0.91      0.91      0.91        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(iris.data,\n",
    "    iris.target, test_size=.3)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print metrics.confusion_matrix(y_train, clf.predict(x_train))\n",
    "print metrics.classification_report(y_train, clf.predict(x_train))\n",
    "\n",
    "print metrics.confusion_matrix(y_test, clf.predict(x_test))\n",
    "print metrics.classification_report(y_test, clf.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the concept of \"pruning\" is not as clear in sklearn (specifically since\n",
    "decision trees are only included as a \"well-known\" model), we can generalize the\n",
    "model by changing the defaults of min_samples_leaf and max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36  0  0]\n",
      " [ 0 34  0]\n",
      " [ 0  2 33]]\n",
      "[[14  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  3 12]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       0.83      0.94      0.88        16\n",
      "          2       0.92      0.80      0.86        15\n",
      "\n",
      "avg / total       0.92      0.91      0.91        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.set_params(min_samples_leaf=4)\n",
    "clf.set_params(max_depth=3)\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print metrics.confusion_matrix(y_train, clf.predict(x_train))\n",
    "print metrics.confusion_matrix(y_test, clf.predict(x_test))\n",
    "\n",
    "print metrics.classification_report(y_test, clf.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.set_params(min_samples_leaf=4)\n",
    "clf.set_params(max_depth=3)\n",
    "clf = clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you install pydot with `pip install pydot` you can also plot the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "with open(\"iris.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've installed graphviz - log into your virtual machine and run:\n",
    "```bash\n",
    "sudo apt-get install graphviz\n",
    "```\n",
    "Before running the following to convert the `iris.dot` into an `iris.png` image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!dot -Tpng iris.dot > iris.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Iris Graphviz Plot](iris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iristree](assets/iristree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ongoing classification assignment _Predicting Bad Used Car Purchases_ has been added to the repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/resources.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handbooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![resource](assets/intro_to_data_mining.png)[Introduction to Data Mining (Ch.4)](http://www-users.cs.umn.edu/~kumar/dmbook/index.php)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Basic Evaluation Measures for Classifier Performance](http://webdocs.cs.ualberta.ca/~eisner/measures.html)\n",
    "* [The Relationship Between Precision-Recall and ROC Curves](https://lirias.kuleuven.be/bitstream/123456789/295592/1/d.)\n",
    "* [Precision recall sensitivity and specificity](http://uberpython.wordpress.com/2012/01/01/precision-recall-sensitivity-and-specificity/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Multivariate_Analysis](http://nbviewer.ipython.org/github/piti118/babar_python_tutorial/blob/master/notebooks/03_Multivariate_Analysis.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
